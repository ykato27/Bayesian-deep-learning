{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, inputDim, hiddenDim, outputDim):\n",
    "        super(Predictor, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size = inputDim,\n",
    "                            hidden_size = hiddenDim,\n",
    "                            batch_first = True)\n",
    "        self.output_layer = nn.Linear(hiddenDim, outputDim)\n",
    "    \n",
    "    def forward(self, inputs, hidden0=None):\n",
    "        output, (hidden, cell) = self.rnn(inputs, hidden0)\n",
    "        output = self.output_layer(output[:, -1, :])\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkDataSet(data_size, data_length=50, freq=60., noise=0.00):\n",
    "    \"\"\"\n",
    "    params\\n\n",
    "    data_size : データセットサイズ\\n\n",
    "    data_length : 各データの時系列長\\n\n",
    "    freq : 周波数\\n\n",
    "    noise : ノイズの振幅\\n\n",
    "    returns\\n\n",
    "    train_x : トレーニングデータ（t=1,2,...,size-1の値)\\n\n",
    "    train_t : トレーニングデータのラベル（t=sizeの値）\\n\n",
    "    \"\"\"\n",
    "    train_x = []\n",
    "    train_t = []\n",
    "\n",
    "    for offset in range(data_size):\n",
    "        train_x.append([[math.sin(2 * math.pi * (offset + i) / freq) + np.random.normal(loc=0.0, scale=noise)] for i in range(data_length)])\n",
    "        train_t.append([math.sin(2 * math.pi * (offset + data_length) / freq)])\n",
    "\n",
    "    return train_x, train_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkRandomBatch(train_x, train_t, batch_size=10):\n",
    "    \"\"\"\n",
    "    train_x, train_tを受け取ってbatch_x, batch_tを返す。\n",
    "    \"\"\"\n",
    "    batch_x = []\n",
    "    batch_t = []\n",
    "\n",
    "    for _ in range(batch_size):\n",
    "        idx = np.random.randint(0, len(train_x) - 1)\n",
    "        batch_x.append(train_x[idx])\n",
    "        batch_t.append(train_t[idx])\n",
    "    \n",
    "    return torch.tensor(batch_x), torch.tensor(batch_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    training_size = 10000\n",
    "    test_size = 1000\n",
    "    epochs_num = 1000\n",
    "    hidden_size = 5\n",
    "    batch_size = 100\n",
    "\n",
    "    train_x, train_t = mkDataSet(training_size)\n",
    "    test_x, test_t = mkDataSet(test_size)\n",
    "\n",
    "    model = Predictor(1, hidden_size, 1)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(epochs_num):\n",
    "        # training\n",
    "        running_loss = 0.0\n",
    "        training_accuracy = 0.0\n",
    "        for i in range(int(training_size / batch_size)):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            data, label = mkRandomBatch(train_x, train_t, batch_size)\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.data.item()\n",
    "            training_accuracy += np.sum(np.abs((output.data - label.data).numpy()) < 0.1)\n",
    "\n",
    "        #test\n",
    "        test_accuracy = 0.0\n",
    "        for i in range(int(test_size / batch_size)):\n",
    "            offset = i * batch_size\n",
    "            data, label = torch.tensor(test_x[offset:offset+batch_size]), torch.tensor(test_t[offset:offset+batch_size])\n",
    "            output = model(data, None)\n",
    "\n",
    "            test_accuracy += np.sum(np.abs((output.data - label.data).numpy()) < 0.1)\n",
    "        \n",
    "        training_accuracy /= training_size\n",
    "        test_accuracy /= test_size\n",
    "\n",
    "        print('%d loss: %.3f, training_accuracy: %.5f, test_accuracy: %.5f' % (\n",
    "            epoch + 1, running_loss, training_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loss: 53.482, training_accuracy: 0.06300, test_accuracy: 0.06700\n",
      "2 loss: 40.792, training_accuracy: 0.07150, test_accuracy: 0.10100\n",
      "3 loss: 25.676, training_accuracy: 0.08950, test_accuracy: 0.11900\n",
      "4 loss: 11.563, training_accuracy: 0.11700, test_accuracy: 0.13600\n",
      "5 loss: 6.217, training_accuracy: 0.12710, test_accuracy: 0.13600\n",
      "6 loss: 4.781, training_accuracy: 0.14500, test_accuracy: 0.15300\n",
      "7 loss: 4.157, training_accuracy: 0.15720, test_accuracy: 0.16900\n",
      "8 loss: 3.808, training_accuracy: 0.16560, test_accuracy: 0.16900\n",
      "9 loss: 3.469, training_accuracy: 0.16980, test_accuracy: 0.18600\n",
      "10 loss: 3.256, training_accuracy: 0.18940, test_accuracy: 0.20200\n",
      "11 loss: 2.998, training_accuracy: 0.20590, test_accuracy: 0.20200\n",
      "12 loss: 2.833, training_accuracy: 0.20790, test_accuracy: 0.20200\n",
      "13 loss: 2.681, training_accuracy: 0.20620, test_accuracy: 0.21900\n",
      "14 loss: 2.520, training_accuracy: 0.21840, test_accuracy: 0.21900\n",
      "15 loss: 2.431, training_accuracy: 0.21940, test_accuracy: 0.21900\n",
      "16 loss: 2.298, training_accuracy: 0.22610, test_accuracy: 0.23500\n",
      "17 loss: 2.182, training_accuracy: 0.22000, test_accuracy: 0.23500\n",
      "18 loss: 2.107, training_accuracy: 0.23580, test_accuracy: 0.21800\n",
      "19 loss: 2.028, training_accuracy: 0.24110, test_accuracy: 0.23500\n",
      "20 loss: 1.976, training_accuracy: 0.23720, test_accuracy: 0.23500\n",
      "21 loss: 1.899, training_accuracy: 0.25510, test_accuracy: 0.25100\n",
      "22 loss: 1.824, training_accuracy: 0.25600, test_accuracy: 0.25100\n",
      "23 loss: 1.779, training_accuracy: 0.27440, test_accuracy: 0.25100\n",
      "24 loss: 1.726, training_accuracy: 0.27850, test_accuracy: 0.30000\n",
      "25 loss: 1.680, training_accuracy: 0.30770, test_accuracy: 0.30100\n",
      "26 loss: 1.618, training_accuracy: 0.33680, test_accuracy: 0.33200\n",
      "27 loss: 1.558, training_accuracy: 0.35950, test_accuracy: 0.36500\n",
      "28 loss: 1.529, training_accuracy: 0.39120, test_accuracy: 0.41300\n",
      "29 loss: 1.513, training_accuracy: 0.37940, test_accuracy: 0.38200\n",
      "30 loss: 1.451, training_accuracy: 0.41550, test_accuracy: 0.44600\n",
      "31 loss: 1.414, training_accuracy: 0.45410, test_accuracy: 0.44900\n",
      "32 loss: 1.379, training_accuracy: 0.48780, test_accuracy: 0.49400\n",
      "33 loss: 1.351, training_accuracy: 0.50630, test_accuracy: 0.52900\n",
      "34 loss: 1.298, training_accuracy: 0.53870, test_accuracy: 0.54500\n",
      "35 loss: 1.277, training_accuracy: 0.55670, test_accuracy: 0.57900\n",
      "36 loss: 1.243, training_accuracy: 0.57390, test_accuracy: 0.57900\n",
      "37 loss: 1.209, training_accuracy: 0.57330, test_accuracy: 0.57900\n",
      "38 loss: 1.183, training_accuracy: 0.59720, test_accuracy: 0.57900\n",
      "39 loss: 1.163, training_accuracy: 0.58390, test_accuracy: 0.57900\n",
      "40 loss: 1.145, training_accuracy: 0.60060, test_accuracy: 0.61200\n",
      "41 loss: 1.110, training_accuracy: 0.60480, test_accuracy: 0.61200\n",
      "42 loss: 1.096, training_accuracy: 0.60800, test_accuracy: 0.61200\n",
      "43 loss: 1.065, training_accuracy: 0.61740, test_accuracy: 0.61200\n",
      "44 loss: 1.062, training_accuracy: 0.61310, test_accuracy: 0.61200\n",
      "45 loss: 1.039, training_accuracy: 0.61690, test_accuracy: 0.61200\n",
      "46 loss: 1.002, training_accuracy: 0.63000, test_accuracy: 0.62900\n",
      "47 loss: 0.963, training_accuracy: 0.65160, test_accuracy: 0.62900\n",
      "48 loss: 0.970, training_accuracy: 0.63360, test_accuracy: 0.64500\n",
      "49 loss: 0.945, training_accuracy: 0.64560, test_accuracy: 0.64500\n",
      "50 loss: 0.932, training_accuracy: 0.65280, test_accuracy: 0.66200\n",
      "51 loss: 0.913, training_accuracy: 0.65630, test_accuracy: 0.64500\n",
      "52 loss: 0.876, training_accuracy: 0.67040, test_accuracy: 0.66200\n",
      "53 loss: 0.867, training_accuracy: 0.66600, test_accuracy: 0.66200\n",
      "54 loss: 0.847, training_accuracy: 0.67010, test_accuracy: 0.66200\n",
      "55 loss: 0.834, training_accuracy: 0.67480, test_accuracy: 0.67800\n",
      "56 loss: 0.814, training_accuracy: 0.68620, test_accuracy: 0.67800\n",
      "57 loss: 0.803, training_accuracy: 0.68240, test_accuracy: 0.67800\n",
      "58 loss: 0.784, training_accuracy: 0.69810, test_accuracy: 0.69500\n",
      "59 loss: 0.775, training_accuracy: 0.69820, test_accuracy: 0.69500\n",
      "60 loss: 0.756, training_accuracy: 0.70010, test_accuracy: 0.69500\n",
      "61 loss: 0.737, training_accuracy: 0.70210, test_accuracy: 0.69500\n",
      "62 loss: 0.728, training_accuracy: 0.70180, test_accuracy: 0.71100\n",
      "63 loss: 0.701, training_accuracy: 0.71980, test_accuracy: 0.71100\n",
      "64 loss: 0.696, training_accuracy: 0.72680, test_accuracy: 0.74500\n",
      "65 loss: 0.681, training_accuracy: 0.75140, test_accuracy: 0.74500\n",
      "66 loss: 0.668, training_accuracy: 0.74870, test_accuracy: 0.74500\n",
      "67 loss: 0.647, training_accuracy: 0.75250, test_accuracy: 0.74500\n",
      "68 loss: 0.631, training_accuracy: 0.76030, test_accuracy: 0.76200\n",
      "69 loss: 0.619, training_accuracy: 0.76460, test_accuracy: 0.77900\n",
      "70 loss: 0.616, training_accuracy: 0.77680, test_accuracy: 0.79600\n",
      "71 loss: 0.598, training_accuracy: 0.79840, test_accuracy: 0.79600\n",
      "72 loss: 0.581, training_accuracy: 0.80280, test_accuracy: 0.79600\n",
      "73 loss: 0.576, training_accuracy: 0.79450, test_accuracy: 0.79600\n",
      "74 loss: 0.551, training_accuracy: 0.80630, test_accuracy: 0.81300\n",
      "75 loss: 0.542, training_accuracy: 0.82060, test_accuracy: 0.81300\n",
      "76 loss: 0.527, training_accuracy: 0.82730, test_accuracy: 0.83000\n",
      "77 loss: 0.522, training_accuracy: 0.84240, test_accuracy: 0.84700\n",
      "78 loss: 0.503, training_accuracy: 0.85200, test_accuracy: 0.84700\n",
      "79 loss: 0.503, training_accuracy: 0.84600, test_accuracy: 0.84700\n",
      "80 loss: 0.488, training_accuracy: 0.85960, test_accuracy: 0.86400\n",
      "81 loss: 0.471, training_accuracy: 0.87420, test_accuracy: 0.88100\n",
      "82 loss: 0.460, training_accuracy: 0.87790, test_accuracy: 0.86400\n",
      "83 loss: 0.449, training_accuracy: 0.89070, test_accuracy: 0.91500\n",
      "84 loss: 0.444, training_accuracy: 0.92500, test_accuracy: 0.93200\n",
      "85 loss: 0.431, training_accuracy: 0.95530, test_accuracy: 0.96600\n",
      "86 loss: 0.421, training_accuracy: 0.96680, test_accuracy: 0.96600\n",
      "87 loss: 0.399, training_accuracy: 0.98100, test_accuracy: 0.96600\n",
      "88 loss: 0.396, training_accuracy: 0.99040, test_accuracy: 1.00000\n",
      "89 loss: 0.389, training_accuracy: 0.99860, test_accuracy: 1.00000\n",
      "90 loss: 0.374, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "91 loss: 0.370, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "92 loss: 0.357, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "93 loss: 0.350, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "94 loss: 0.340, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "95 loss: 0.333, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "96 loss: 0.320, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "97 loss: 0.312, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "98 loss: 0.302, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "99 loss: 0.294, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "100 loss: 0.288, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "101 loss: 0.283, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "102 loss: 0.269, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "103 loss: 0.263, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "104 loss: 0.256, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "105 loss: 0.246, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "106 loss: 0.243, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "107 loss: 0.231, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "108 loss: 0.228, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "109 loss: 0.221, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "110 loss: 0.215, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "111 loss: 0.208, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "112 loss: 0.198, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "113 loss: 0.196, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "114 loss: 0.189, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "115 loss: 0.183, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "116 loss: 0.177, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "117 loss: 0.172, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "118 loss: 0.166, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "119 loss: 0.161, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "120 loss: 0.158, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "121 loss: 0.154, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "122 loss: 0.150, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "123 loss: 0.145, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "124 loss: 0.143, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "125 loss: 0.137, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "126 loss: 0.132, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "127 loss: 0.128, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "128 loss: 0.126, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "129 loss: 0.123, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "130 loss: 0.121, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "131 loss: 0.115, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "132 loss: 0.112, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "133 loss: 0.110, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "134 loss: 0.110, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "135 loss: 0.106, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "136 loss: 0.103, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "137 loss: 0.100, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "138 loss: 0.099, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "139 loss: 0.095, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "140 loss: 0.094, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "141 loss: 0.092, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "142 loss: 0.090, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "143 loss: 0.088, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "144 loss: 0.086, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "145 loss: 0.084, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "146 loss: 0.084, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "147 loss: 0.080, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "148 loss: 0.080, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "149 loss: 0.079, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "150 loss: 0.078, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "151 loss: 0.075, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "152 loss: 0.074, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "153 loss: 0.073, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "154 loss: 0.073, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "155 loss: 0.071, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "156 loss: 0.069, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "157 loss: 0.068, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "158 loss: 0.068, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "159 loss: 0.066, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "160 loss: 0.064, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "161 loss: 0.064, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "162 loss: 0.064, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "163 loss: 0.062, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "164 loss: 0.062, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "165 loss: 0.060, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "166 loss: 0.059, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "167 loss: 0.058, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "168 loss: 0.059, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "169 loss: 0.058, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "170 loss: 0.057, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "171 loss: 0.055, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "172 loss: 0.057, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "173 loss: 0.056, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "174 loss: 0.055, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "175 loss: 0.055, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "176 loss: 0.052, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "177 loss: 0.052, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "178 loss: 0.052, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "179 loss: 0.052, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "180 loss: 0.051, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "181 loss: 0.051, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "182 loss: 0.050, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "183 loss: 0.050, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "184 loss: 0.050, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "185 loss: 0.049, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "186 loss: 0.049, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "187 loss: 0.048, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "188 loss: 0.048, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "189 loss: 0.047, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "190 loss: 0.047, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "191 loss: 0.047, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "192 loss: 0.046, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "193 loss: 0.045, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "194 loss: 0.046, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "195 loss: 0.045, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "196 loss: 0.045, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "197 loss: 0.045, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "198 loss: 0.044, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "199 loss: 0.044, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "200 loss: 0.044, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "201 loss: 0.043, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "202 loss: 0.043, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "203 loss: 0.043, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "204 loss: 0.043, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "205 loss: 0.043, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "206 loss: 0.042, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "207 loss: 0.042, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "208 loss: 0.042, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "209 loss: 0.041, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "210 loss: 0.041, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "211 loss: 0.041, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "212 loss: 0.041, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "213 loss: 0.040, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "214 loss: 0.040, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "215 loss: 0.040, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "216 loss: 0.040, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "217 loss: 0.040, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "218 loss: 0.039, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "219 loss: 0.039, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "220 loss: 0.038, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "221 loss: 0.038, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "222 loss: 0.038, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "223 loss: 0.038, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "224 loss: 0.038, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "225 loss: 0.038, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "226 loss: 0.037, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "227 loss: 0.037, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "228 loss: 0.037, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "229 loss: 0.037, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "230 loss: 0.037, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "231 loss: 0.036, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "232 loss: 0.036, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "233 loss: 0.036, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "234 loss: 0.036, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "235 loss: 0.036, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "236 loss: 0.035, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "237 loss: 0.036, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "238 loss: 0.036, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "239 loss: 0.035, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "240 loss: 0.035, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "241 loss: 0.034, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "242 loss: 0.035, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "243 loss: 0.034, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "244 loss: 0.034, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "245 loss: 0.034, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "246 loss: 0.034, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "247 loss: 0.034, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "248 loss: 0.034, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "249 loss: 0.034, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "250 loss: 0.033, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "251 loss: 0.033, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "252 loss: 0.033, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "253 loss: 0.032, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "254 loss: 0.033, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "255 loss: 0.033, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "256 loss: 0.032, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "257 loss: 0.032, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "258 loss: 0.033, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "259 loss: 0.032, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "260 loss: 0.032, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "261 loss: 0.032, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "262 loss: 0.031, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "263 loss: 0.032, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "264 loss: 0.031, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "265 loss: 0.031, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "266 loss: 0.031, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "267 loss: 0.031, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "268 loss: 0.031, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "269 loss: 0.031, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "270 loss: 0.031, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "271 loss: 0.031, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "272 loss: 0.030, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "273 loss: 0.030, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "274 loss: 0.030, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "275 loss: 0.029, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "276 loss: 0.030, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "277 loss: 0.030, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "278 loss: 0.029, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "279 loss: 0.029, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "280 loss: 0.029, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "281 loss: 0.029, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "282 loss: 0.029, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "283 loss: 0.029, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "284 loss: 0.029, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "285 loss: 0.029, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "286 loss: 0.029, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "287 loss: 0.028, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "288 loss: 0.028, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "289 loss: 0.028, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "290 loss: 0.028, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "291 loss: 0.028, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "292 loss: 0.028, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "293 loss: 0.028, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "294 loss: 0.028, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "295 loss: 0.028, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "296 loss: 0.027, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "297 loss: 0.028, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "298 loss: 0.028, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "299 loss: 0.027, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "300 loss: 0.027, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "301 loss: 0.027, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "302 loss: 0.027, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "303 loss: 0.027, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "304 loss: 0.027, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "305 loss: 0.026, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "306 loss: 0.027, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "307 loss: 0.026, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "308 loss: 0.027, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "309 loss: 0.026, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "310 loss: 0.026, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "311 loss: 0.026, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "312 loss: 0.026, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "313 loss: 0.026, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "314 loss: 0.026, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "315 loss: 0.026, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "316 loss: 0.026, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "317 loss: 0.025, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "318 loss: 0.025, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "319 loss: 0.026, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "320 loss: 0.026, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "321 loss: 0.026, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "322 loss: 0.025, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "323 loss: 0.025, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "324 loss: 0.025, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "325 loss: 0.025, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "326 loss: 0.025, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "327 loss: 0.025, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "328 loss: 0.025, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "329 loss: 0.024, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "330 loss: 0.024, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "331 loss: 0.024, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "332 loss: 0.024, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "333 loss: 0.024, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "334 loss: 0.024, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "335 loss: 0.024, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "336 loss: 0.024, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "337 loss: 0.024, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "338 loss: 0.024, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "339 loss: 0.024, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "340 loss: 0.024, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "341 loss: 0.023, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "342 loss: 0.023, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "343 loss: 0.023, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "344 loss: 0.023, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "345 loss: 0.024, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "346 loss: 0.023, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "347 loss: 0.023, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "348 loss: 0.023, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "349 loss: 0.023, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "350 loss: 0.023, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "351 loss: 0.023, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "352 loss: 0.023, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "353 loss: 0.023, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "354 loss: 0.023, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "355 loss: 0.022, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "356 loss: 0.022, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "357 loss: 0.022, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "358 loss: 0.022, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "359 loss: 0.022, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "360 loss: 0.022, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "361 loss: 0.022, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "362 loss: 0.022, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "363 loss: 0.021, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "364 loss: 0.022, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "365 loss: 0.022, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "366 loss: 0.021, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "367 loss: 0.022, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "368 loss: 0.022, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "369 loss: 0.021, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "370 loss: 0.021, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "371 loss: 0.021, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "372 loss: 0.021, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "373 loss: 0.021, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "374 loss: 0.021, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "375 loss: 0.021, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "376 loss: 0.021, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "377 loss: 0.021, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "378 loss: 0.021, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "379 loss: 0.021, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "380 loss: 0.021, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "381 loss: 0.021, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "382 loss: 0.020, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "383 loss: 0.020, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "384 loss: 0.021, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "385 loss: 0.020, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "386 loss: 0.020, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "387 loss: 0.020, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "388 loss: 0.020, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "389 loss: 0.020, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "390 loss: 0.020, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "391 loss: 0.020, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "392 loss: 0.020, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "393 loss: 0.020, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "394 loss: 0.020, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "395 loss: 0.020, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "396 loss: 0.020, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "397 loss: 0.020, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "398 loss: 0.019, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "399 loss: 0.019, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "400 loss: 0.020, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "401 loss: 0.019, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "402 loss: 0.019, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "403 loss: 0.019, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "404 loss: 0.019, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "405 loss: 0.019, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "406 loss: 0.019, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "407 loss: 0.019, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "408 loss: 0.019, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "409 loss: 0.019, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "410 loss: 0.019, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "411 loss: 0.019, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "412 loss: 0.019, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "413 loss: 0.019, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "414 loss: 0.019, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "415 loss: 0.019, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "416 loss: 0.019, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "417 loss: 0.018, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "418 loss: 0.019, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "419 loss: 0.018, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "420 loss: 0.018, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "421 loss: 0.019, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "422 loss: 0.018, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "423 loss: 0.018, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "424 loss: 0.018, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "425 loss: 0.018, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "426 loss: 0.018, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "427 loss: 0.018, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "428 loss: 0.018, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "429 loss: 0.018, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "430 loss: 0.018, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "431 loss: 0.018, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "432 loss: 0.018, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "433 loss: 0.018, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "434 loss: 0.017, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "435 loss: 0.017, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "436 loss: 0.017, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "437 loss: 0.018, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "438 loss: 0.017, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "439 loss: 0.018, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "440 loss: 0.017, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "441 loss: 0.017, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "442 loss: 0.017, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "443 loss: 0.017, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "444 loss: 0.017, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "445 loss: 0.017, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "446 loss: 0.017, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "447 loss: 0.017, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "448 loss: 0.017, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "449 loss: 0.017, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "450 loss: 0.017, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "451 loss: 0.017, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "452 loss: 0.017, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "453 loss: 0.017, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "454 loss: 0.017, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "455 loss: 0.016, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "456 loss: 0.017, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "457 loss: 0.017, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "458 loss: 0.016, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "459 loss: 0.016, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "460 loss: 0.017, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "461 loss: 0.016, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "462 loss: 0.016, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "463 loss: 0.016, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "464 loss: 0.016, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "465 loss: 0.016, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "466 loss: 0.016, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "467 loss: 0.016, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "468 loss: 0.016, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "469 loss: 0.016, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "470 loss: 0.016, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "471 loss: 0.016, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "472 loss: 0.016, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "473 loss: 0.016, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "474 loss: 0.016, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "475 loss: 0.016, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "476 loss: 0.016, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "477 loss: 0.016, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "478 loss: 0.016, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "479 loss: 0.016, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "480 loss: 0.016, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "481 loss: 0.016, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "482 loss: 0.016, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "483 loss: 0.015, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "484 loss: 0.015, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "485 loss: 0.016, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "486 loss: 0.015, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "487 loss: 0.015, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "488 loss: 0.015, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "489 loss: 0.015, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "490 loss: 0.015, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "491 loss: 0.015, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "492 loss: 0.015, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "493 loss: 0.015, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "494 loss: 0.015, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "495 loss: 0.015, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "496 loss: 0.015, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "497 loss: 0.015, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "498 loss: 0.015, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "499 loss: 0.015, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "500 loss: 0.015, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "501 loss: 0.015, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "502 loss: 0.015, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "503 loss: 0.015, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "504 loss: 0.015, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "505 loss: 0.014, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "506 loss: 0.015, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "507 loss: 0.015, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "508 loss: 0.014, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "509 loss: 0.015, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "510 loss: 0.015, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "511 loss: 0.015, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "512 loss: 0.015, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "513 loss: 0.014, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "514 loss: 0.014, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "515 loss: 0.014, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "516 loss: 0.014, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "517 loss: 0.014, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "518 loss: 0.014, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "519 loss: 0.014, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "520 loss: 0.014, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "521 loss: 0.014, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "522 loss: 0.014, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "523 loss: 0.014, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "524 loss: 0.014, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "525 loss: 0.014, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "526 loss: 0.014, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "527 loss: 0.014, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "528 loss: 0.014, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "529 loss: 0.014, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "530 loss: 0.014, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "531 loss: 0.014, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "532 loss: 0.014, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "533 loss: 0.014, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "534 loss: 0.014, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "535 loss: 0.014, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "536 loss: 0.014, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "537 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "538 loss: 0.014, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "539 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "540 loss: 0.014, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "541 loss: 0.014, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "542 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "543 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "544 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "545 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "546 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "547 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "548 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "549 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "550 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "551 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "552 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "553 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "554 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "555 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "556 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "557 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "558 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "559 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "560 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "561 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "562 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "563 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "564 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "565 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "566 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "567 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "568 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "569 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "570 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "571 loss: 0.013, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "572 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "573 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "574 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "575 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "576 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "577 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "578 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "579 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "580 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "581 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "582 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "583 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "584 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "585 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "586 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "587 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "588 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "589 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "590 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "591 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "592 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "593 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "594 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "595 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "596 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "597 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "598 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "599 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "600 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "601 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "602 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "603 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "604 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "605 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "606 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "607 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "608 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "609 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "610 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "611 loss: 0.012, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "612 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "613 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "614 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "615 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "616 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "617 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "618 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "619 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "620 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "621 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "622 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "623 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "624 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "625 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "626 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "627 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "628 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "629 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "630 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "631 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "632 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "633 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "634 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "635 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "636 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "637 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "638 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "639 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "640 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "641 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "642 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "643 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "644 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "645 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "646 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "647 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "648 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "649 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "650 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "651 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "652 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "653 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "654 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "655 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "656 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "657 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "658 loss: 0.011, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "659 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "660 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "661 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "662 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "663 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "664 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "665 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "666 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "667 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "668 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "669 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "670 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "671 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "672 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "673 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "674 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "675 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "676 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "677 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "678 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "679 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "680 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "681 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "682 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "683 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "684 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "685 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "686 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "687 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "688 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "689 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "690 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "691 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "692 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "693 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "694 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "695 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "696 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "697 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "698 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "699 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "700 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "701 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "702 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "703 loss: 0.010, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "704 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "705 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "706 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "707 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "708 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "709 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "710 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "711 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "712 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "713 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "714 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "715 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "716 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "717 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "718 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "719 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "720 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "721 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "722 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "723 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "724 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "725 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "726 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "727 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "728 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "729 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "730 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "731 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "732 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "733 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "734 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "735 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "736 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "737 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "738 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "739 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "740 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "741 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "742 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "743 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "744 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "745 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "746 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "747 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "748 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "749 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "750 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "751 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "752 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "753 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "754 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "755 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "756 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "757 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "758 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "759 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "760 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "761 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "762 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "763 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "764 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "765 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "766 loss: 0.009, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "767 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "768 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "769 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "770 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "771 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "772 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "773 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "774 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "775 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "776 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "777 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "778 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "779 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "780 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "781 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "782 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "783 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "784 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "785 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "786 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "787 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "788 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "789 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "790 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "791 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "792 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "793 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "794 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "795 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "796 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "797 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "798 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "799 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "800 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "801 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "802 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "803 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "804 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "805 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "806 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "807 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "808 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "809 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "810 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "811 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "812 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "813 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "814 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "815 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "816 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "817 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "818 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "819 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "820 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "821 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "822 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "823 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "824 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "825 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "826 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "827 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "828 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "829 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "830 loss: 0.008, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "831 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "832 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "833 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "834 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "835 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "836 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "837 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "838 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "839 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "840 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "841 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "842 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "843 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "844 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "845 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "846 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "847 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "848 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "849 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "850 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "851 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "852 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "853 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "854 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "855 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "856 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "857 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "858 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "859 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "860 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "861 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "862 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "863 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "864 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "865 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "866 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "867 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "868 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "869 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "870 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "871 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "872 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "873 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "874 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "875 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "876 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "877 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "878 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "879 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "880 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "881 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "882 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "883 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "884 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "885 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "886 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "887 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "888 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "889 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "890 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "891 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "892 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "893 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "894 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "895 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "896 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "897 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "898 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "899 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "900 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "901 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "902 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "903 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "904 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "905 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "906 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "907 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "908 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "909 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "910 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "911 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "912 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "913 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "914 loss: 0.007, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "915 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "916 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "917 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "918 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "919 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "920 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "921 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "922 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "923 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "924 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "925 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "926 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "927 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "928 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "929 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "930 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "931 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "932 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "933 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "934 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "935 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "936 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "937 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "938 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "939 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "940 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "941 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "942 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "943 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "944 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "945 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "946 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "947 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "948 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "949 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "950 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "951 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "952 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "953 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "954 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "955 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "956 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "957 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "958 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "959 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "960 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "961 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "962 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "963 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "964 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "965 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "966 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "967 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "968 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "969 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "970 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "971 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "972 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "973 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "974 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "975 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "976 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "977 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "978 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "979 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "980 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "981 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "982 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "983 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "984 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "985 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "986 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "987 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "988 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "989 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "990 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "991 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "992 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "993 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "994 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "995 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "996 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "997 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "998 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "999 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
      "1000 loss: 0.006, training_accuracy: 1.00000, test_accuracy: 1.00000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
